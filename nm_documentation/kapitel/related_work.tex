\chapter{Verwandte Literatur}

Modelle mit externem Speicher konnten in der Vergangenheit bereits für verschiedenste RL Probleme erfolgreich eingesetzt werden. Der folgende Abschnitt soll einen Überblick über die in diesem Bereich bereits geleistete Forschungsarbeit geben.

Graves et al. präsentierten im Jahr 2016 den sogenannten Differentiable Neural Computer (DNC) \cite{DNC}. Dessen Desgin ist inspiriert von herkömmlichen Computern, d.h. es findet eine Trennung zwischen den Berechnungen und dem Speicher statt. Somit besteht der DNC aus einem Controller, ähnlich einer CPU, und einem externen Speicher in Analogie zum RAM. Der Controller ist realisiert in Form eines rekurrenten Neuronalen Netzes. Dieses erhält die Eingabe und generiert unter zuhilfenahme des Speichers die Ausgabe. Dabei erfolgt der Speicherzugriff mittels differenzierbarer Lese- und Schreiboperationen, die wiederum mittels eines Attention Mechanismus bestimmen inwiefern jede Speicherposition in die jeweilige Operation involviert ist. Bedingt durch die volldifferenzierbare Struktur des DNC können sowohl die Operationen als auch die Generierung der Ausgabe durch den Controller mittels gradientenbasierten Verfahren erlernt werden. Die Autoren zeigten, dass das DNC verschiedenste algorithmische Aufgaben lösen kann wie beispielsweise die Suche des kürzesten Pfades in einem Graphen.

Oh et al. stellten im Jahr 2016 mehrere sogenannte Memory Network (MemNN) Architekturen vor \cite{MemNN}. Die grundsätzliche Struktur ähnelt der des DNC, allerdings verfügen sie über keinen besonders ausgeklügelten Schreiboperator. Infolgedessen enthält der Speicher einfach die letzten $N$ Zustände, wobei diese in Form kodierter Paare, bestehend aus Schlüssel und Wert, vorliegen. Die Kodierung der Zustände übernimmt ein Neuronales Faltungsnetz. Sowohl zur Abfrage des Speicherinhalts als auch zur Berechnung der Ausgabe wird zunächst ein sogenannter Context Vektor berechnet. Dieser wird wiederum von der Leseoperation, die auf dem Soft Attention Mechanismus basiert, für den Speicherzugriff verwendet. Das Ergebnis der Leseoperation wird zusammen mit dem Context Vektor benutzt, um die Ausgabe des Modells zu generieren. Auf diese Weise kann das Modell lernen, wie es relevante Informationen aus seinem Speicher liest bzw. nutzt. Jedoch wird es durch die Größe seines Speichers limitiert. Im Rahmen des Papers wurde demonstriert, dass die MemNN Architekturen verschiedene auf Labyrinthen basierte Problemstellungen erfolgreich meistern.

Gupta et al. publizierten im Jahr 2017 ein Modell zur Navigation von Robotern \cite{MappingPlanning}. Dazu besteht es aus einem Mapper und einem Planer. Der Mapper erhält als Eingabe Bilder der Umgebung. Basierend auf diesen und auf seinem internen Speicher schätzt er den freien Raum um den Roboter und dessen Position und Orientierung. Diese Schätzung wird im nächsten Schritt vom Planer verabeitet, der wiederum auf dem Verfahren der Value Iteration basiert. Darüber hinaus arbeitet der Planer hierarchisch. Zunächst wird ein Plan erstellt auf einer räumlich stark herunter skalierten Umgebung. Dieser Vorgang wird solange auf immer weniger herunter skalierten Umgebungen wiederholt, bis schlussendlich die ursprüngliche Umgebung erreicht wird. Das Modell wurde genutzt, um den Roboter zu verschiedenen Zielen zu navigieren. Dabei können die Ziele entweder in Form von Koordinaten vorliegen oder in Form semantischer Beschreibungen wie besipielseweise dem Auffinden eines bestimmten Gegenstandes.

Beck et al. veröffentlichten im Jahr 2020 einen weiteren Ansatz, den sie als Aggregated Memory for Reinforcement Learning (AMRL) bezeichneten \cite{AMRL}. Diese Klasse von Modellen soll besonders robust gegenüber rauschbehafteten Eingangsdaten sein. Dabei wird der Eingang des Modells zuerst von einem Feed-Forward Netz und anschließend von einem LSTM prozessiert. Dessen Ausgang wiederum wird gemeinsam mit dem internen Speicher vom Herzstück des Modells, der sogenannten Aggregator Funktion, verarbeitet. Diese sollen zum einen das Rauschen unterdrücken und zum anderen den Gradienten stabilisieren, d.h. das Problem verschwindender Gradienten beheben. Abschließend berechnet ein weiteres Feed-Forward Netz auf Basis der Aggregator Funktion die Ausgabe des Modells. Das AMRL konnte seine Überlegenheit gegenüber anderen Modellen in verschiedenen labyrinthbasierten Aufgaben, die sowohl in 2- wie auch in 3-dimensionalen Umgebungen stattfanden, unter Beweis stellen.
